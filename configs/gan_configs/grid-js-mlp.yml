gan_config:
  dp: false
  dataset: 
    name: gaussians_grid
    params:
      sigma: &sigma 0.05
      n_modes: 25
      sample_size: 25600
  train_transform:
    Normalize:
      mean: &mean [0., 0.]
      std: &std [1.415, 1.415]
  prior: normal
  eval: true
  generator:
    name: MLPGenerator
    params:
      mean: *mean
      std: *std
      # z_dim: 2
      # n_layers: 4
      # n_hid: 100
      # n_out: 2
      z_dim: 64
      n_layers: 4
      n_hid: 128
      n_out: 2
    ckpt_path: checkpoints/mlp/G_25_gauss_js.pth
  discriminator:
    name: MLPDiscriminator
    params:
      mean: *mean
      std: *std
      n_layers: 4
      n_hid: 128
      n_in: 2
    ckpt_path: checkpoints/mlp/D_25_gauss_js.pth



# train_batch_size: 256
# n_epochs: &n_epochs 500 #1000
# criterion_g: &criterion_g
#   name: JensenLoss
# criterion_d: &criterion_d
#   name: JensenLoss
# optimizer_g: &optimizer_g
#   name: Adam
#   params:
#     lr: 0.0001
#     betas: [0.5, 0.99]
# optimizer_d: &optimizer_d
#   name: Adam
#   params:
#     lr: 0.0003
#     betas: [0.5, 0.99]
# trainer_kwargs: &trainer_kwargs
#   n_dis: 1
#   sample_size: 5000
#   grad_acc_steps: 1
#   sample_steps: &sample_steps 5

# range: &range [-2.5, 2.5]

train_batch_size: 256
# n_epochs: &n_epochs 500
n_train_iters: &n_train_iters 50000
criterion_g: &criterion_g
  name: JensenNSLoss
criterion_d: &criterion_d
  name: JensenNSLoss
optimizer_g: &optimizer_g
  name: Adam
  params:
    lr: 0.0001
    betas: [0.5, 0.999]
optimizer_d: &optimizer_d
  name: Adam
  params:
    lr: 0.0001
    betas: [0.5, 0.999]
trainer_kwargs: &trainer_kwargs
  n_dis: 1 #3
  sample_size: 25000
  grad_acc_steps: 1
  sample_steps: &sample_steps 1
  r1_coef: 0.05

range: &range [-2.5, 2.5]

